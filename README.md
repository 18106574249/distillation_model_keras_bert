Knowledge Distillation
----
代码是在kaggle kernel上面写的（ipynb格式），迁移下来时还没来得及调试，直接运行可能会报错。  <br>
kaggle kernel代码地址：https://www.kaggle.com/duolaaa/weibo-distil-student-layers3  <br>
代码实现了CNN、BiLSTM、Bert(3layers)对Bert(12layers)模型的蒸馏。


### 参考文献
Patient Knowledge Distillation for BERT Model Compression <br>
Distilling the Knowledge in a Neural Network
