Knowledge Distillation
----
代码是在kaggle kernel上面写的（ipynb格式），迁移下来时还没来得及调试，直接运行可能会报错。  <br>
kaggle kernel代码地址：https://www.kaggle.com/duolaaa/weibo-distil-student-layers3  <br>
分别使用CNN、BiLSTM、Bert(3layers)作为Bert(12layers)的蒸馏模型。


### 参考文献
Patient Knowledge Distillation for BERT Model Compression <br>
Distilling the Knowledge in a Neural Network
